{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs and RNNs\n",
    "\n",
    "\n",
    "In this unit we will talk about teh Convolution Neural Network (CNN) and Recurrent Neural Network (RNN), as well as introducing how you can build them through using the module pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolution\n",
    "\n",
    "In order to understand a CNN it is worth spending some time talking about the convolution. \n",
    "\n",
    "The idea of a convolution is to pass one system through another. If a system is though of as a function, it is the idea of taking every output of each function for every input and combining them in some way. In our case we can simulate this by passing a kernel over our image. On initialization the kernel has random values, but the values can be learned through the training process. \n",
    "\n",
    "The kernel we convolve with can be any value, and the size, stride and padding is a hyper parameter a model can be trained on. \n",
    "\n",
    "The kernel acts as a way to consolidate information as it passes throughout the model. The output of the kernel and the data is known as a feature map. This creates a \"high level features\" on our feature map. It can be thought of as finding higher level patterns in our data. The more kernels we have the \"features\" we tend to learn.\n",
    "\n",
    "Large kernels can learn more global patterns or high level details, while small kernels can help learn more detailed patterns. \n",
    "\n",
    "As the feature map information is passed through multiple kernels, a single value later down the network represents many values closer to the input of the feature map.\n",
    "\n",
    "https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html\n",
    "https://www.striveworks.com/blog/demystifying-computer-vision-the-power-of-convolution-in-neural-networks#:~:text=What%20Is%20Convolution%3F\n",
    "https://www.youtube.com/watch?v=kebSR2Ph7zg\n",
    "\n",
    "Interesting Kernels: https://en.wikipedia.org/wiki/Kernel_(image_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "Pytorch is a machine learning library created by Facebook/Meta, it is open source and created under the Linux Foundation. This along with TensorFlow is one of the most powerful open source machine learning libraries available. You can learn more about pytorch in the link below.\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "\n",
    "Pytorch as many of the same steps as scikit learn, however since pytorch's power comes from its ability to be more granular, there is a bit more setup involved. However, this gives us much more flexibility on our data. \n",
    "\n",
    "1. Import the data\n",
    "2. Setup the dataset class\n",
    "3. Feed into a data loader\n",
    "4. Build your model\n",
    "5. Train your model\n",
    "6. Evaluate the results\n",
    "\n",
    "There are many many resources online, so I encourage you to look at examples! \n",
    "\n",
    "Pytorch relies heavily on you to create classes, so if you are a bit rusty please check out online resources for class creation in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inspired heavily from https://www.kaggle.com/code/tiiktak/fashion-mnist-with-alexnet-in-pytorch-92-accuracy\n",
    "# 1. Import the data\n",
    "# Data source: https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download\n",
    "\n",
    "import zipfile\n",
    "import torch\n",
    "# Unzip the Data\n",
    "with zipfile.ZipFile('./fashion-mnist_test.csv.zip', 'r') as zip:\n",
    "    zip.extractall('fashion-mnist_test')\n",
    "\n",
    "with zipfile.ZipFile('./fashion-mnist_train.csv.zip', 'r') as zip:\n",
    "    zip.extractall('fashion-mnist_train')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fashion_train_csv = pd.read_csv('./fashion-mnist_train/fashion-mnist_train.csv')\n",
    "fashion_test_csv = pd.read_csv('./fashion-mnist_test/fashion-mnist_test.csv')\n",
    "\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      2       0       0       0       0       0       0       0       0   \n",
      "1      9       0       0       0       0       0       0       0       0   \n",
      "2      6       0       0       0       0       0       0       0       5   \n",
      "3      0       0       0       0       1       2       0       0       0   \n",
      "4      3       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0        30        43         0   \n",
      "3       0  ...         3         0         0         0         0         1   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Setup for Datasets\n",
    "print(fashion_train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):        \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        self.transform = transform\n",
    "        \n",
    "        label, image = [], []\n",
    "        \n",
    "        # Is there a better way to do this :)\n",
    "        for i in self.fashion_MNIST:\n",
    "            label.append(i[0])\n",
    "            image.append(i[1:])\n",
    "        self.labels = np.asarray(label)\n",
    "        self.images = np.asarray(image).reshape(-1, 28, 28).astype('float32')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]      \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # transfrom the numpy array to PIL image before the transform function\n",
    "            pil_image = Image.fromarray(np.uint8(image)) \n",
    "            image = self.transform(pil_image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "from torchvision import transforms, datasets\n",
    "# Feed into our dataset\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fashion_train_dataset = FashionDataset(fashion_train_csv, preprocess)\n",
    "fashion_test_dataset = FashionDataset(fashion_test_csv, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Feed into a data loader\n",
    "BATCH_SIZE = 64\n",
    "fashion_train_loader = DataLoader(fashion_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_test_dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Data\n",
    "import matplotlib.pyplot as plt\n",
    "for batch, labels in fashion_train_loader:\n",
    "    print(batch.shape)\n",
    "    print(labels.shape)\n",
    "    plt.imshow(batch[0].squeeze())\n",
    "    print( CLASS_NAMES[labels[0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 484])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "import torch.nn as nn\n",
    "# There is a lot of nuance here\n",
    "\n",
    "\n",
    "class fashion_mnist_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fashion_mnist_model, self).__init__()\n",
    "        # Create the layers we want to use\n",
    "        # Made these up for this example\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=3, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(in_features=22 * 22, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x : torch.Tensor):\n",
    "        # Use the layers we created to do a feed forward of our data\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Look at all of these hyperparameters!\n",
    "model = fashion_mnist_model()\n",
    "for batch, labels in fashion_train_loader:\n",
    "    out = model(batch)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Example\n",
    "\n",
    "One useful aspect of pytorch is we can use existing models provided. Granted these are academic model, but they allow us to use the structures right out of the box. We can then train on data without setup for the model. We still need to setup unpacking our data and loading it into datasets, but this way we dont need to create a model object from scratch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet, AlexNet_Weights\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch\n",
    "# Get the dataset ready\n",
    "# Wnat to use mnist dataset I think \n",
    "\n",
    "# TODO: Get the test dataset out and \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=3, ),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean=[0.5], std=[0.25]),\n",
    "    \n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, \n",
    "                                           transform=preprocess)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Note if you had your own data you would want to use \n",
    "#torch.utils.data.Dataset OR torch.utils.data.TensorDataset(X, y)\n",
    "#torch.utils.data.DataLoader(input_dataset, batch_size, etc)\n",
    "\n",
    "# For torch.utils.data.Dataset() you will need to make a class \n",
    "# you must make a __init__, __len__ and __getitem__ methods before it can be used\n",
    "# See example here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "import matplotlib.pyplot as plt\n",
    "for batch, label in train_loader:\n",
    "    print(batch.shape)\n",
    "    print(label[0])\n",
    "    plt.imshow(batch[0].squeeze()[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "# Load the model\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "# Set it to eval mode to prevent learning\n",
    "model.eval()\n",
    "\n",
    "for batch, label in train_loader:\n",
    "\n",
    "    output = model(batch)\n",
    "    print(output[0])\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
