{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs and RNNs\n",
    "\n",
    "<iframe src=\"https://giphy.com/embed/22eVpVYpRhaE0\" width=\"480\" height=\"274\" style=\"\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/dreamworks-shrek-donkey-22eVpVYpRhaE0\">via GIPHY</a></p>\n",
    "\n",
    "In this unit we will talk about the Convolution Neural Network (CNN) and Recurrent Neural Network (RNN), as well as introducing how you can build them through using the module pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Convolution\n",
    "\n",
    "In order to understand a CNN it is worth spending some time talking about the convolution. \n",
    "\n",
    "The idea of a convolution is to pass one system through another to get the characteristics of how they interact. If a system is thought of as a function, it is the idea of taking every output of each function for every input and combining them in some way. In our case we can simulate this by passing a kernel over our image. On initialization the kernel has random values, but the values can be learned through the training process. \n",
    "\n",
    "In our examples we will be used a 2 dimensional kernel, but kernels can be 1, 3, and more dimensions. \n",
    "\n",
    "A 2d kernel can be represented like a grid shown below, it has a shape, and each block in the grid contains a numeric value:\n",
    "\n",
    "<insert image of grid with random values and shapes>\n",
    "\n",
    "As a kernel is passed over data, the value in each kernel cell will be mathematically combined with a value of data in some way. Usually we multiply them. Then once that is done for every kernel value and corresponding data value the collection of output is combined together, usually by adding, and spit out as a single data value. \n",
    "\n",
    "Image of Kernel Convolution:\n",
    "\n",
    "<img src=\"./images/kernel_convolution.png\" width=\"600\">\n",
    "\n",
    "Here is a great example animation by wikipedia:\n",
    "https://en.wikipedia.org/wiki/File:2D_Convolution_Animation.gif\n",
    "\n",
    "How far the kernel is moved after each step is called the stride. \n",
    "\n",
    "One important aspect to note and not shown in the animation, is that if a kernel is passed over an image the output size will be reduced based on the kernel's size. \n",
    "\n",
    "The reduction in size can be expressed by this equation: \n",
    "\n",
    "n x n convolved with f x f = (n - f + 1) x (n - f + 1)\n",
    "\n",
    "The kernel acts as a way to consolidate information as it passes throughout the model. The output of the kernel and the data is known as a feature map. This creates a \"high level features\" on our feature map. It can be thought of as finding higher level patterns in our data. The more kernels we have the \"features\" we tend to learn.\n",
    "\n",
    "Large kernels can learn more global patterns or high level details, while small kernels can help learn more detailed patterns. \n",
    "\n",
    "As the feature map information is passed through multiple kernels, a single value later down the network represents many values closer to the input of the feature map.\n",
    "\n",
    "Alexnet Architecture for Reference:\n",
    "\n",
    "<img src=\"./images/alexnet_arch.png\" width=\"600\">\n",
    "\n",
    "Alexnet is a convocational neural net that takes in image data. Here the diagram shows the internal architecture of how it is built. In the diagram the orange squares represent data and its dimensions. The yellow boxes show the kernels and their sizes. The orange boxes change in size based on the number of kernels that are used - these are stacked on top of each other to get the new size. The activation function is usually applied at the output of the kernel convolution. The \"FC\" layers are the \"fully connected layers\" like the MLP we discussed in a previous lecture.\n",
    "\n",
    "Note at the time this model was considered to be so big they did all the training on two GPUs separately, the architecture on the two GPUs can be seen in the paper in the citations.\n",
    "\n",
    "<img src=\"./images/kernel_output_stacking.png\" width=\"600\">\n",
    "\n",
    "One great aspect of kernels is the variety of uses they have outside of machine learning too, here are some pretty cool examples:\n",
    "Interesting Kernels: https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "\n",
    "\n",
    "Citations and Useful Links on Convolutions and Kernels:\n",
    "- https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html\n",
    "- https://www.striveworks.com/blog/demystifying-computer-vision-the-power-of-convolution-in-neural-networks#:~:text=What%20Is%20Convolution%3F\n",
    "- https://www.youtube.com/watch?v=kebSR2Ph7zg\n",
    "- https://medium.com/@abhishekjainindore24/all-about-convolutions-kernels-features-in-cnn-c656616390a1\n",
    "- https://stackoverflow.com/questions/56652204/pytorch-convolution-in-channels-and-out-channels-meaning\n",
    "- https://paperswithcode.com/method/alexnet\n",
    "- https://viso.ai/deep-learning/alexnet/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Pytorch - a CNN Example\n",
    "\n",
    "Pytorch is a machine learning library created by Facebook/Meta, it is open source and created under the Linux Foundation. This along with TensorFlow is one of the most powerful open source machine learning libraries available. You can learn more about pytorch in the link below.\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "\n",
    "Pytorch as many of the same steps as scikit learn, however since pytorch's power comes from its ability to be more granular, there is a bit more setup involved. However, this gives us much more flexibility on our data. \n",
    "\n",
    "1. Import the data\n",
    "2. Setup the dataset class\n",
    "3. Feed into a data loader\n",
    "4. Build your model\n",
    "5. Train your model\n",
    "6. Evaluate the results\n",
    "\n",
    "There are many many resources online, so I encourage you to look at examples! \n",
    "\n",
    "Pytorch relies heavily on you to create classes, so if you are a bit rusty please check out online resources for class creation in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inspired heavily from https://www.kaggle.com/code/tiiktak/fashion-mnist-with-alexnet-in-pytorch-92-accuracy\n",
    "# 1. Import the data\n",
    "# Data source: https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download\n",
    "\n",
    "import zipfile\n",
    "import torch\n",
    "# Unzip the Data\n",
    "with zipfile.ZipFile('./fashion-mnist_test.csv.zip', 'r') as zip:\n",
    "    zip.extractall('fashion-mnist_test')\n",
    "\n",
    "with zipfile.ZipFile('./fashion-mnist_train.csv.zip', 'r') as zip:\n",
    "    zip.extractall('fashion-mnist_train')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fashion_train_csv = pd.read_csv('./fashion-mnist_train/fashion-mnist_train.csv')\n",
    "fashion_test_csv = pd.read_csv('./fashion-mnist_test/fashion-mnist_test.csv')\n",
    "\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0      2       0       0       0       0       0       0       0       0   \n",
      "1      9       0       0       0       0       0       0       0       0   \n",
      "2      6       0       0       0       0       0       0       0       5   \n",
      "3      0       0       0       0       1       2       0       0       0   \n",
      "4      3       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0        30        43         0   \n",
      "3       0  ...         3         0         0         0         0         1   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# Setup for Datasets\n",
    "print(fashion_train_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):        \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        self.transform = transform\n",
    "        \n",
    "        label, image = [], []\n",
    "        \n",
    "        # Is there a better way to do this :)\n",
    "        for i in self.fashion_MNIST:\n",
    "            label.append(i[0])\n",
    "            image.append(i[1:])\n",
    "        self.labels = np.asarray(label)\n",
    "        self.images = np.asarray(image).reshape(-1, 28, 28).astype('float32')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]      \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # transfrom the numpy array to PIL image before the transform function\n",
    "            pil_image = Image.fromarray(np.uint8(image)) \n",
    "            image = self.transform(pil_image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "from torchvision import transforms, datasets\n",
    "# Feed into our dataset\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5))\n",
    "])\n",
    "\n",
    "fashion_train_dataset = FashionDataset(fashion_train_csv, preprocess)\n",
    "fashion_test_dataset = FashionDataset(fashion_test_csv, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Feed into a data loader\n",
    "BATCH_SIZE = 4\n",
    "fashion_train_loader = DataLoader(fashion_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_test_dataset, batch_size=BATCH_SIZE, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag\n"
     ]
    }
   ],
   "source": [
    "# Visualize Data\n",
    "import matplotlib.pyplot as plt\n",
    "for batch, labels in fashion_train_loader:\n",
    "    print(batch.shape)\n",
    "    print(labels.shape)\n",
    "    # Unnormize batch\n",
    "    batch = batch/2 + 0.5\n",
    "    plt.figure()\n",
    "    plt.imshow(batch[0].squeeze())\n",
    "    plt.show()\n",
    "    print( CLASS_NAMES[int(labels[0])])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "import torch.nn as nn\n",
    "# There is a lot of nuance here\n",
    "\n",
    "\n",
    "class fashion_mnist_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fashion_mnist_model, self).__init__()\n",
    "        # Create the layers we want to use\n",
    "        # Made these up for this example\n",
    "\n",
    "        # Convolutional layer what has 1 input channel - since our images are grayscale\n",
    "        # Note we do not have to specify the image size here. \n",
    "        # It will then create 10 kernels 3x3 - this determines our number of out channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=3, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3)\n",
    "        # Output will be 10 here for the 10 classes. We want to see the \n",
    "        # probability of what the image input is for each class.\n",
    "        self.fc1 = nn.Linear(in_features=22 * 22, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x : torch.Tensor):\n",
    "        # Use the layers we created to do a feed forward of our data\n",
    "        # This is the actual model architecture\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Look at all of these hyperparameters we can change!\n",
    "\n",
    "model = fashion_mnist_model()\n",
    "model.eval() # This locks our model and prevents training\n",
    "for batch, labels in fashion_train_loader:\n",
    "    out = model(batch)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "import datetime\n",
    "# There are a few extra steps in training our model, such as \n",
    "# Determining our loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Determine the number of epochs\n",
    "EPOCHS = 3\n",
    "# (Optional but very helpful) tracking loss\n",
    "running_loss = 0\n",
    "last_lost = 0\n",
    "running_loss_log = []\n",
    "epoch_loss = 0\n",
    "epoch_loss_log = []\n",
    "# Make model\n",
    "fashion_model = fashion_mnist_model()\n",
    "# Determining our optimizer\n",
    "optimizer = torch.optim.SGD(fashion_model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  batch 1000 loss: 10.168432836160063\n",
      "  batch 2000 loss: 6.513258492503082\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for i, batch in enumerate(fashion_train_loader):\n",
    "        inputs, labels = batch\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions\n",
    "        outputs = fashion_model(inputs)\n",
    "        # Calculate loss on expected data and gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        # Report loss\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 100\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss_log.append(last_loss)\n",
    "            running_loss = 0\n",
    "    epoch_loss_log.append(epoch_loss)\n",
    "    epoch_loss = 0\n",
    "    time_now = datetime.datetime.now()\n",
    "    # model_path = './model_{}_{}_{}_{}'.format(time_now.hour, time_now.minute, time_now.second, epoch)\n",
    "    # torch.save(fashion_model.state_dict(), model_path)\n",
    "# It is also a good idea to save a model locally, that way it is\n",
    "# not lost if your computer turns off, or you can continue training\n",
    "# it later \n",
    "\n",
    "# Model can be loaded by:\n",
    "#saved_model = fashion_mnist_model()\n",
    "#saved_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Model\n",
    "plt.figure()\n",
    "plt.title(\"Fashion Model Loss\")\n",
    "plt.plot(running_loss_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Mode\n",
    "fashion_model.eval() # Prevents training\n",
    "total_samples = 0\n",
    "total_correct = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(fashion_test_loader):\n",
    "        inputs, labels = batch\n",
    "        outputs = fashion_model(inputs)\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        correct += torch.sum(predicted_labels == labels)\n",
    "        total += predicted_labels.shape[0]\n",
    "\n",
    "accuracy = correct/total\n",
    "print(\"Accuracy of model:\", accuracy.item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "The idea of a RNN is there may be information in previous layers that may be lost throughout the model as it passes through each layer. \n",
    "\n",
    "The general architecture can be seen here:\n",
    "\n",
    "\n",
    "An RNN may take different forms, and depending on the application you may have better results with different architectures. It is worth noting the different relationships that exists for RNNs like one to one, one to many, many to many, and many to one. \n",
    "\n",
    "Lets build a RNN with Pytorch pulling heavily from the citation below.\n",
    "\n",
    "Citations:\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "- https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP From Scratch - Classifying Names\n",
    "\n",
    "Ihe tutorial example we will try to predict the language of origin for a name. This idea could be extended to trying to predict the country code for a phone number on a phone based on inputs to a first name.\n",
    "\n",
    "What are some positive uses for this model? Anything creative?\n",
    "What are some negative implications of this models? Can this model be used in malicious ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import zipfile\n",
    "import torch\n",
    "# Unzip the Data\n",
    "with zipfile.ZipFile('./name_data.zip', 'r') as zip:\n",
    "    zip.extractall('name_data')\n",
    "\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(\"Found: \", findFiles('./name_data/data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('./name_data/data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Turn the names into tensors\n",
    "# here a letter is indexed based on \"one-hot vector\"\n",
    "# For example, if there were 3 total letters, a, b, c then \n",
    "# a would be [1, 0, 0]\n",
    "# b would be [0, 1, 0]\n",
    "# c would be [0, 0, 1]\n",
    "\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "print(rnn.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with one \"channel\" or letter\n",
    "# This is an example of checking if your model\n",
    "# can accept the expected input\n",
    "input = letterToTensor('A')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optimization step\n",
    "# And could be precomputed before putting into model\n",
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)\n",
    "# The output is the likelihood of the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for training\n",
    "\n",
    "# Helper function to get output\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick way to get a training example\n",
    "\n",
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '| line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network \n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying on user input:\n",
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')\n",
    "predict('Trenton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet Example (Optional example)\n",
    "\n",
    "One useful aspect of pytorch is we can use existing models provided. Granted these are academic model, but they allow us to use the structures right out of the box. We can then train on data without setup for the model. We still need to setup unpacking our data and loading it into datasets, but this way we dont need to create a model object from scratch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import AlexNet, AlexNet_Weights\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch\n",
    "# Get the dataset ready\n",
    "# Wnat to use mnist dataset I think \n",
    "\n",
    "# TODO: Get the test dataset out and \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Grayscale(num_output_channels=3, ),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean=[0.5], std=[0.25]),\n",
    "    \n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, \n",
    "                                           transform=preprocess)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Note if you had your own data you would want to use \n",
    "#torch.utils.data.Dataset OR torch.utils.data.TensorDataset(X, y)\n",
    "#torch.utils.data.DataLoader(input_dataset, batch_size, etc)\n",
    "\n",
    "# For torch.utils.data.Dataset() you will need to make a class \n",
    "# you must make a __init__, __len__ and __getitem__ methods before it can be used\n",
    "# See example here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "import matplotlib.pyplot as plt\n",
    "for batch, label in train_loader:\n",
    "    print(batch.shape)\n",
    "    print(label[0])\n",
    "    plt.imshow(batch[0].squeeze()[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "# Load the model\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "# Set it to eval mode to prevent learning\n",
    "model.eval()\n",
    "\n",
    "for batch, label in train_loader:\n",
    "\n",
    "    output = model(batch)\n",
    "    print(output[0])\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, download=True)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(labels.shape)\n",
    "    print(labels[0])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unit-8-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
